{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wSEeTsO1gftS-A38KcirnEZ1Mr5EhRD6",
      "authorship_tag": "ABX9TyPhepjMd58lH15iy7LCXvSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gio-Choi/Stock_Regime_Analysis/blob/main/MLP_model_split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgE2Mek_wOxH",
        "outputId": "4fb2e5b3-736a-4089-b3a4-7814915d5419"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone_Dataset_Next.csv')"
      ],
      "metadata": {
        "id": "nKvFHHtHvpyW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "meXu-G00wgSr",
        "outputId": "c4e0e756-7e79-4939-ba03-5399ea462f60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0        ^GSPC  VIXCLS  CPIAUCSL     M2SL  FEDFUNDS  T10Y2Y  \\\n",
              "0    2004-04-01  1132.170044   16.65   187.400   6191.2      1.00    2.26   \n",
              "1    2004-05-03  1117.489990   16.62   188.200   6268.1      1.00    2.19   \n",
              "2    2004-06-01  1121.199951   16.30   188.900   6270.5      1.03    2.11   \n",
              "3    2004-07-01  1128.939941   15.20   189.100   6284.3      1.26    1.93   \n",
              "4    2004-08-02  1106.619995   15.37   189.200   6310.6      1.43    1.82   \n",
              "..          ...          ...     ...       ...      ...       ...     ...   \n",
              "196  2020-08-03  3294.610107   24.28   259.580  18357.4      0.10    0.45   \n",
              "197  2020-09-01  3526.649902   26.12   260.190  18575.2      0.09    0.55   \n",
              "198  2020-10-01  3380.800049   26.70   260.352  18735.7      0.09    0.54   \n",
              "199  2020-11-02  3310.239990   37.13   260.721  18969.8      0.09    0.71   \n",
              "200  2020-12-01  3662.449951   20.77   261.564  19124.7      0.09    0.75   \n",
              "\n",
              "      AAA  UNRATE  PCUOMFGOMFG  USPHCI     IQ  S&P_pct  \n",
              "0    5.73     5.6        141.8   91.28  103.7      0.0  \n",
              "1    6.04     5.6        143.3   91.52  104.1      0.0  \n",
              "2    6.01     5.6        142.9   91.73  103.4      1.0  \n",
              "3    5.82     5.5        143.2   91.95  103.9      1.0  \n",
              "4    5.65     5.4        143.7   92.15  103.4      0.0  \n",
              "..    ...     ...          ...     ...    ...      ...  \n",
              "196  2.25     8.4        193.0  121.22  122.1      1.0  \n",
              "197  2.31     7.9        192.9  121.97  122.8      1.0  \n",
              "198  2.35     6.9        193.7  122.93  123.0      0.0  \n",
              "199  2.30     6.7        194.5  123.41  124.0      0.0  \n",
              "200  2.26     6.7        196.7  123.66  125.5      1.0  \n",
              "\n",
              "[201 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5bcb993-cf3f-4524-a90f-a90328e55380\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>^GSPC</th>\n",
              "      <th>VIXCLS</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "      <th>M2SL</th>\n",
              "      <th>FEDFUNDS</th>\n",
              "      <th>T10Y2Y</th>\n",
              "      <th>AAA</th>\n",
              "      <th>UNRATE</th>\n",
              "      <th>PCUOMFGOMFG</th>\n",
              "      <th>USPHCI</th>\n",
              "      <th>IQ</th>\n",
              "      <th>S&amp;P_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-04-01</td>\n",
              "      <td>1132.170044</td>\n",
              "      <td>16.65</td>\n",
              "      <td>187.400</td>\n",
              "      <td>6191.2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>2.26</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.6</td>\n",
              "      <td>141.8</td>\n",
              "      <td>91.28</td>\n",
              "      <td>103.7</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-05-03</td>\n",
              "      <td>1117.489990</td>\n",
              "      <td>16.62</td>\n",
              "      <td>188.200</td>\n",
              "      <td>6268.1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>2.19</td>\n",
              "      <td>6.04</td>\n",
              "      <td>5.6</td>\n",
              "      <td>143.3</td>\n",
              "      <td>91.52</td>\n",
              "      <td>104.1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-06-01</td>\n",
              "      <td>1121.199951</td>\n",
              "      <td>16.30</td>\n",
              "      <td>188.900</td>\n",
              "      <td>6270.5</td>\n",
              "      <td>1.03</td>\n",
              "      <td>2.11</td>\n",
              "      <td>6.01</td>\n",
              "      <td>5.6</td>\n",
              "      <td>142.9</td>\n",
              "      <td>91.73</td>\n",
              "      <td>103.4</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-07-01</td>\n",
              "      <td>1128.939941</td>\n",
              "      <td>15.20</td>\n",
              "      <td>189.100</td>\n",
              "      <td>6284.3</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.93</td>\n",
              "      <td>5.82</td>\n",
              "      <td>5.5</td>\n",
              "      <td>143.2</td>\n",
              "      <td>91.95</td>\n",
              "      <td>103.9</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-08-02</td>\n",
              "      <td>1106.619995</td>\n",
              "      <td>15.37</td>\n",
              "      <td>189.200</td>\n",
              "      <td>6310.6</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.82</td>\n",
              "      <td>5.65</td>\n",
              "      <td>5.4</td>\n",
              "      <td>143.7</td>\n",
              "      <td>92.15</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>2020-08-03</td>\n",
              "      <td>3294.610107</td>\n",
              "      <td>24.28</td>\n",
              "      <td>259.580</td>\n",
              "      <td>18357.4</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.45</td>\n",
              "      <td>2.25</td>\n",
              "      <td>8.4</td>\n",
              "      <td>193.0</td>\n",
              "      <td>121.22</td>\n",
              "      <td>122.1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>3526.649902</td>\n",
              "      <td>26.12</td>\n",
              "      <td>260.190</td>\n",
              "      <td>18575.2</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.55</td>\n",
              "      <td>2.31</td>\n",
              "      <td>7.9</td>\n",
              "      <td>192.9</td>\n",
              "      <td>121.97</td>\n",
              "      <td>122.8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>2020-10-01</td>\n",
              "      <td>3380.800049</td>\n",
              "      <td>26.70</td>\n",
              "      <td>260.352</td>\n",
              "      <td>18735.7</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.54</td>\n",
              "      <td>2.35</td>\n",
              "      <td>6.9</td>\n",
              "      <td>193.7</td>\n",
              "      <td>122.93</td>\n",
              "      <td>123.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>2020-11-02</td>\n",
              "      <td>3310.239990</td>\n",
              "      <td>37.13</td>\n",
              "      <td>260.721</td>\n",
              "      <td>18969.8</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.71</td>\n",
              "      <td>2.30</td>\n",
              "      <td>6.7</td>\n",
              "      <td>194.5</td>\n",
              "      <td>123.41</td>\n",
              "      <td>124.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>3662.449951</td>\n",
              "      <td>20.77</td>\n",
              "      <td>261.564</td>\n",
              "      <td>19124.7</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.75</td>\n",
              "      <td>2.26</td>\n",
              "      <td>6.7</td>\n",
              "      <td>196.7</td>\n",
              "      <td>123.66</td>\n",
              "      <td>125.5</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>201 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5bcb993-cf3f-4524-a90f-a90328e55380')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5bcb993-cf3f-4524-a90f-a90328e55380 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5bcb993-cf3f-4524-a90f-a90328e55380');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6_Ahjfhx6TR",
        "outputId": "f488e4ec-f87f-44f4-9014-f4b078b5dedd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.9.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.7/dist-packages (from scikeras) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=3 in /usr/local/lib/python3.7/dist-packages (from scikeras) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=0.21->scikeras) (3.0.9)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "Ukt8YM-L4YQU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.values\n",
        "dataset[:,2:12]"
      ],
      "metadata": {
        "id": "5SSUJY9xyNJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8485d8fd-7bdc-4ad3-f74b-932544348548"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16.65, 187.4, 6191.2, ..., 141.8, 91.28, 103.7],\n",
              "       [16.62, 188.2, 6268.1, ..., 143.3, 91.52, 104.1],\n",
              "       [16.3, 188.9, 6270.5, ..., 142.9, 91.73, 103.4],\n",
              "       ...,\n",
              "       [26.7, 260.352, 18735.7, ..., 193.7, 122.93, 123.0],\n",
              "       [37.13, 260.721, 18969.8, ..., 194.5, 123.41, 124.0],\n",
              "       [20.77, 261.564, 19124.7, ..., 196.7, 123.66, 125.5]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(dataset, test_size = 0.2, random_state = 42)\n",
        "print(\"훈련 데이터 개수 : \", len(train_data))\n",
        "print(\"테스트 데이터 개수 : \", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9KcTpsEKQPF",
        "outputId": "bcc3ae6d-881a-4f92-92d1-59ca08643ec5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터 개수 :  160\n",
            "테스트 데이터 개수 :  41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into input (X) and output (Y) variables\n",
        "X = train_data[:,2:12].astype(float)\n",
        "Y = train_data[:,-1]"
      ],
      "metadata": {
        "id": "xCwbbCvIyBfG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_data[:,2:12].astype(float)\n",
        "Y_test = test_data[:,-1]\n",
        "encoder_test = LabelEncoder()\n",
        "encoder_test.fit(Y_test)\n",
        "Y_test = encoder_test.transform(Y_test)"
      ],
      "metadata": {
        "id": "mnZygsSPNZYm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiN4v1hx14Ej",
        "outputId": "a5c3b831-b661-4771-94b4-dc7d485411ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,\n",
              "       1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0,\n",
              "       1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
              "       1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0,\n",
              "       1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0,\n",
              "       0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0,\n",
              "       0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0,\n",
              "       1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0,\n",
              "       1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0,\n",
              "       0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0,\n",
              "       1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0,\n",
              "       0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
              "       1.0, 1.0, 1.0, 1.0], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)"
      ],
      "metadata": {
        "id": "V8Lxs5Mjzj7w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, input_shape=(10,), activation='elu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(32, activation='elu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(BatchNormalization())  \n",
        "  model.add(Dense(16, activation='elu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(4, activation='elu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "4btqLfcd9-DX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_model(model=create_model(), to_file='NN.png')\n",
        "plot_model(model=create_model(), to_file='NN.png', show_shapes=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gYHhDf6tsRht",
        "outputId": "539d2e7c-3d98-44b9-a8d7-f677d62baf6e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증데이터손실(val_loss)가 증가하면 과적합 징후이므로, 검증데이터 손실이 3회 증가하면 early stopping\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=60) #verbose로 학습이 되는 과정을 볼 수 있다, patience는 개선이 없더라도 기다려줄 횟수\n",
        "# 검증 데이터의 정확도가 이전보다 좋아진 경우에만 모델 저장 \n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only= True)"
      ],
      "metadata": {
        "id": "eBv9XrAzMrco"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# estimators = []\n",
        "# estimators.append(('standardize', StandardScaler()))\n",
        "# estimators.append(('mlp', KerasClassifier(model=create_model, epochs=100, batch_size=8, verbose=1)))\n",
        "# pipeline = Pipeline(estimators)\n",
        "# 제외 callbacks=[mc],\n",
        "model = create_model()\n",
        "model.fit(X, encoded_Y, epochs=400, callbacks=[mc], batch_size= 8, validation_split= 0.2)"
      ],
      "metadata": {
        "id": "a8mY90Pa0BjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e388ddaa-e119-415f-90c3-a703d673758e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 1.0676 - accuracy: 0.4625 \n",
            "Epoch 1: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 2s 18ms/step - loss: 1.0711 - accuracy: 0.3984 - val_loss: 2.2818 - val_accuracy: 0.3125\n",
            "Epoch 2/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7400 - accuracy: 0.5455\n",
            "Epoch 2: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7366 - accuracy: 0.5078 - val_loss: 1.0566 - val_accuracy: 0.3125\n",
            "Epoch 3/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7622 - accuracy: 0.5000\n",
            "Epoch 3: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7814 - accuracy: 0.4844 - val_loss: 0.8480 - val_accuracy: 0.3438\n",
            "Epoch 4/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6406 - accuracy: 0.5909\n",
            "Epoch 4: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6728 - accuracy: 0.5547 - val_loss: 0.7296 - val_accuracy: 0.4375\n",
            "Epoch 5/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7541 - accuracy: 0.4773\n",
            "Epoch 5: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7419 - accuracy: 0.4688 - val_loss: 0.7013 - val_accuracy: 0.5625\n",
            "Epoch 6/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6737 - accuracy: 0.6250\n",
            "Epoch 6: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6698 - accuracy: 0.6172 - val_loss: 0.6822 - val_accuracy: 0.6250\n",
            "Epoch 7/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6728 - accuracy: 0.6364\n",
            "Epoch 7: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6695 - accuracy: 0.6172 - val_loss: 0.6704 - val_accuracy: 0.6562\n",
            "Epoch 8/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7037 - accuracy: 0.5909\n",
            "Epoch 8: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6932 - accuracy: 0.6172 - val_loss: 0.6856 - val_accuracy: 0.6250\n",
            "Epoch 9/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6811 - accuracy: 0.6125\n",
            "Epoch 9: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6977 - accuracy: 0.6016 - val_loss: 0.6829 - val_accuracy: 0.6250\n",
            "Epoch 10/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6698 - accuracy: 0.5909\n",
            "Epoch 10: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6674 - accuracy: 0.6016 - val_loss: 0.6735 - val_accuracy: 0.6562\n",
            "Epoch 11/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7218 - accuracy: 0.5500\n",
            "Epoch 11: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6956 - accuracy: 0.5859 - val_loss: 0.6768 - val_accuracy: 0.6250\n",
            "Epoch 12/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7263 - accuracy: 0.5250\n",
            "Epoch 12: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7285 - accuracy: 0.5391 - val_loss: 0.6626 - val_accuracy: 0.6562\n",
            "Epoch 13/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7185 - accuracy: 0.5568\n",
            "Epoch 13: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7134 - accuracy: 0.5469 - val_loss: 0.6598 - val_accuracy: 0.6875\n",
            "Epoch 14/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6661 - accuracy: 0.5909\n",
            "Epoch 14: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6622 - accuracy: 0.5938 - val_loss: 0.6467 - val_accuracy: 0.6875\n",
            "Epoch 15/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6244 - accuracy: 0.6705\n",
            "Epoch 15: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6510 - accuracy: 0.6406 - val_loss: 0.6410 - val_accuracy: 0.6875\n",
            "Epoch 16/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6801 - accuracy: 0.5909\n",
            "Epoch 16: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6742 - accuracy: 0.5859 - val_loss: 0.6319 - val_accuracy: 0.6875\n",
            "Epoch 17/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6849 - accuracy: 0.6364\n",
            "Epoch 17: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7035 - accuracy: 0.6172 - val_loss: 0.6271 - val_accuracy: 0.6875\n",
            "Epoch 18/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6804 - accuracy: 0.6023\n",
            "Epoch 18: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6836 - accuracy: 0.6016 - val_loss: 0.6324 - val_accuracy: 0.6875\n",
            "Epoch 19/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6630 - accuracy: 0.6364\n",
            "Epoch 19: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6738 - accuracy: 0.6172 - val_loss: 0.6323 - val_accuracy: 0.6875\n",
            "Epoch 20/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6977 - accuracy: 0.5278\n",
            "Epoch 20: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6799 - accuracy: 0.5938 - val_loss: 0.6300 - val_accuracy: 0.6875\n",
            "Epoch 21/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7495 - accuracy: 0.5455\n",
            "Epoch 21: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7159 - accuracy: 0.5781 - val_loss: 0.6280 - val_accuracy: 0.6875\n",
            "Epoch 22/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6482 - accuracy: 0.6364\n",
            "Epoch 22: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.6406 - val_loss: 0.6339 - val_accuracy: 0.6875\n",
            "Epoch 23/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7095 - accuracy: 0.5750\n",
            "Epoch 23: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6771 - accuracy: 0.5938 - val_loss: 0.6443 - val_accuracy: 0.6875\n",
            "Epoch 24/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6904 - accuracy: 0.5625\n",
            "Epoch 24: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.5625 - val_loss: 0.6413 - val_accuracy: 0.6875\n",
            "Epoch 25/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6556 - accuracy: 0.6136\n",
            "Epoch 25: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6668 - accuracy: 0.6172 - val_loss: 0.6202 - val_accuracy: 0.6875\n",
            "Epoch 26/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6841 - accuracy: 0.6023\n",
            "Epoch 26: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6599 - accuracy: 0.6406 - val_loss: 0.6127 - val_accuracy: 0.6875\n",
            "Epoch 27/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6682 - accuracy: 0.6250\n",
            "Epoch 27: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6541 - accuracy: 0.6328 - val_loss: 0.6110 - val_accuracy: 0.6875\n",
            "Epoch 28/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7103 - accuracy: 0.5750\n",
            "Epoch 28: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7001 - accuracy: 0.5703 - val_loss: 0.6116 - val_accuracy: 0.6875\n",
            "Epoch 29/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6409 - accuracy: 0.6500\n",
            "Epoch 29: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6422 - accuracy: 0.6484 - val_loss: 0.6092 - val_accuracy: 0.6875\n",
            "Epoch 30/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6997 - accuracy: 0.5250\n",
            "Epoch 30: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7032 - accuracy: 0.5469 - val_loss: 0.6170 - val_accuracy: 0.6875\n",
            "Epoch 31/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6641 - accuracy: 0.6250\n",
            "Epoch 31: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6569 - accuracy: 0.5938 - val_loss: 0.6199 - val_accuracy: 0.6875\n",
            "Epoch 32/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7161 - accuracy: 0.5795\n",
            "Epoch 32: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7176 - accuracy: 0.5547 - val_loss: 0.6200 - val_accuracy: 0.6875\n",
            "Epoch 33/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6588 - accuracy: 0.6364\n",
            "Epoch 33: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6672 - accuracy: 0.6016 - val_loss: 0.6161 - val_accuracy: 0.6875\n",
            "Epoch 34/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6791 - accuracy: 0.5795\n",
            "Epoch 34: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6970 - accuracy: 0.5625 - val_loss: 0.6133 - val_accuracy: 0.7188\n",
            "Epoch 35/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7011 - accuracy: 0.5000\n",
            "Epoch 35: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6696 - accuracy: 0.5859 - val_loss: 0.6234 - val_accuracy: 0.6875\n",
            "Epoch 36/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6368 - accuracy: 0.6591\n",
            "Epoch 36: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6779 - accuracy: 0.6094 - val_loss: 0.6257 - val_accuracy: 0.6562\n",
            "Epoch 37/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6731 - accuracy: 0.5682\n",
            "Epoch 37: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6972 - accuracy: 0.5469 - val_loss: 0.6153 - val_accuracy: 0.6562\n",
            "Epoch 38/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7065 - accuracy: 0.5568\n",
            "Epoch 38: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6954 - accuracy: 0.5938 - val_loss: 0.6200 - val_accuracy: 0.6875\n",
            "Epoch 39/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6765 - accuracy: 0.6250\n",
            "Epoch 39: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6815 - accuracy: 0.6094 - val_loss: 0.6213 - val_accuracy: 0.6875\n",
            "Epoch 40/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6396 - accuracy: 0.6364\n",
            "Epoch 40: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6553 - accuracy: 0.5781 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
            "Epoch 41/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6612 - accuracy: 0.6477\n",
            "Epoch 41: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6674 - accuracy: 0.6250 - val_loss: 0.6090 - val_accuracy: 0.6875\n",
            "Epoch 42/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6751 - accuracy: 0.5875\n",
            "Epoch 42: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6616 - accuracy: 0.6172 - val_loss: 0.6148 - val_accuracy: 0.6875\n",
            "Epoch 43/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6816 - accuracy: 0.5795\n",
            "Epoch 43: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6721 - accuracy: 0.5859 - val_loss: 0.6304 - val_accuracy: 0.6562\n",
            "Epoch 44/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6727 - accuracy: 0.6136\n",
            "Epoch 44: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6962 - accuracy: 0.5938 - val_loss: 0.6402 - val_accuracy: 0.5625\n",
            "Epoch 45/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6529 - accuracy: 0.6477\n",
            "Epoch 45: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6581 - accuracy: 0.6484 - val_loss: 0.6767 - val_accuracy: 0.5625\n",
            "Epoch 46/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6709 - accuracy: 0.6023\n",
            "Epoch 46: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6577 - accuracy: 0.6094 - val_loss: 0.6817 - val_accuracy: 0.5625\n",
            "Epoch 47/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6554 - accuracy: 0.5682\n",
            "Epoch 47: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6523 - accuracy: 0.5938 - val_loss: 0.6859 - val_accuracy: 0.5625\n",
            "Epoch 48/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6908 - accuracy: 0.5568\n",
            "Epoch 48: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6748 - accuracy: 0.6016 - val_loss: 0.7085 - val_accuracy: 0.5625\n",
            "Epoch 49/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6997 - accuracy: 0.5682\n",
            "Epoch 49: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6956 - accuracy: 0.5469 - val_loss: 0.6997 - val_accuracy: 0.5312\n",
            "Epoch 50/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6266 - accuracy: 0.6750\n",
            "Epoch 50: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6771 - accuracy: 0.6172 - val_loss: 0.7212 - val_accuracy: 0.5625\n",
            "Epoch 51/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7000 - accuracy: 0.5375\n",
            "Epoch 51: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6826 - accuracy: 0.5781 - val_loss: 0.7479 - val_accuracy: 0.5625\n",
            "Epoch 52/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6468 - accuracy: 0.6023\n",
            "Epoch 52: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6631 - accuracy: 0.5938 - val_loss: 0.7065 - val_accuracy: 0.5625\n",
            "Epoch 53/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6947 - accuracy: 0.5500\n",
            "Epoch 53: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6727 - accuracy: 0.5703 - val_loss: 0.6836 - val_accuracy: 0.5312\n",
            "Epoch 54/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6509 - accuracy: 0.5341\n",
            "Epoch 54: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6455 - accuracy: 0.5625 - val_loss: 0.7912 - val_accuracy: 0.5625\n",
            "Epoch 55/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7393 - accuracy: 0.5625\n",
            "Epoch 55: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7051 - accuracy: 0.5703 - val_loss: 0.7447 - val_accuracy: 0.5625\n",
            "Epoch 56/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7013 - accuracy: 0.5909\n",
            "Epoch 56: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6920 - accuracy: 0.5938 - val_loss: 0.7343 - val_accuracy: 0.5625\n",
            "Epoch 57/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6705 - accuracy: 0.6250\n",
            "Epoch 57: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6630 - accuracy: 0.6250 - val_loss: 0.9560 - val_accuracy: 0.3125\n",
            "Epoch 58/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6535 - accuracy: 0.6136\n",
            "Epoch 58: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6490 - accuracy: 0.6328 - val_loss: 1.4614 - val_accuracy: 0.3125\n",
            "Epoch 59/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6539 - accuracy: 0.6250\n",
            "Epoch 59: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6552 - accuracy: 0.6250 - val_loss: 2.0716 - val_accuracy: 0.3125\n",
            "Epoch 60/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6270 - accuracy: 0.6250\n",
            "Epoch 60: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6373 - accuracy: 0.6328 - val_loss: 2.0594 - val_accuracy: 0.3125\n",
            "Epoch 61/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6565 - accuracy: 0.5875\n",
            "Epoch 61: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6484 - accuracy: 0.6172 - val_loss: 1.8804 - val_accuracy: 0.3125\n",
            "Epoch 62/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6706 - accuracy: 0.6364\n",
            "Epoch 62: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6567 - accuracy: 0.6484 - val_loss: 2.2507 - val_accuracy: 0.3125\n",
            "Epoch 63/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6600 - accuracy: 0.6364\n",
            "Epoch 63: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6624 - accuracy: 0.6250 - val_loss: 1.8706 - val_accuracy: 0.3125\n",
            "Epoch 64/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6683 - accuracy: 0.6250\n",
            "Epoch 64: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6706 - accuracy: 0.6172 - val_loss: 1.6391 - val_accuracy: 0.3125\n",
            "Epoch 65/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7119 - accuracy: 0.5795\n",
            "Epoch 65: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7083 - accuracy: 0.5859 - val_loss: 1.7524 - val_accuracy: 0.3125\n",
            "Epoch 66/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6884 - accuracy: 0.5795\n",
            "Epoch 66: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6878 - accuracy: 0.5703 - val_loss: 1.4454 - val_accuracy: 0.3125\n",
            "Epoch 67/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7421 - accuracy: 0.5375\n",
            "Epoch 67: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7004 - accuracy: 0.5938 - val_loss: 1.4138 - val_accuracy: 0.3438\n",
            "Epoch 68/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6670 - accuracy: 0.5682\n",
            "Epoch 68: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6465 - accuracy: 0.6328 - val_loss: 1.2170 - val_accuracy: 0.4375\n",
            "Epoch 69/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6818 - accuracy: 0.5795\n",
            "Epoch 69: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6860 - accuracy: 0.5547 - val_loss: 1.8111 - val_accuracy: 0.3125\n",
            "Epoch 70/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6799 - accuracy: 0.5909\n",
            "Epoch 70: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6809 - accuracy: 0.5859 - val_loss: 1.9320 - val_accuracy: 0.3125\n",
            "Epoch 71/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7032 - accuracy: 0.5795\n",
            "Epoch 71: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7021 - accuracy: 0.5859 - val_loss: 1.2387 - val_accuracy: 0.4375\n",
            "Epoch 72/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6295 - accuracy: 0.6625\n",
            "Epoch 72: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6588 - accuracy: 0.6172 - val_loss: 1.0981 - val_accuracy: 0.5625\n",
            "Epoch 73/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6460 - accuracy: 0.6136\n",
            "Epoch 73: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6722 - accuracy: 0.5938 - val_loss: 1.4050 - val_accuracy: 0.3750\n",
            "Epoch 74/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7087 - accuracy: 0.5682\n",
            "Epoch 74: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6829 - accuracy: 0.5859 - val_loss: 1.9379 - val_accuracy: 0.3125\n",
            "Epoch 75/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6477 - accuracy: 0.6375\n",
            "Epoch 75: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6406 - accuracy: 0.6719 - val_loss: 2.9218 - val_accuracy: 0.3125\n",
            "Epoch 76/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6715 - accuracy: 0.5795\n",
            "Epoch 76: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6777 - accuracy: 0.5547 - val_loss: 1.8752 - val_accuracy: 0.3125\n",
            "Epoch 77/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6686 - accuracy: 0.6125\n",
            "Epoch 77: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6526 - accuracy: 0.6328 - val_loss: 2.0468 - val_accuracy: 0.3125\n",
            "Epoch 78/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6504 - accuracy: 0.6591\n",
            "Epoch 78: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6496 - accuracy: 0.6562 - val_loss: 1.8848 - val_accuracy: 0.3125\n",
            "Epoch 79/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6509 - accuracy: 0.6250\n",
            "Epoch 79: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6497 - accuracy: 0.6328 - val_loss: 1.5303 - val_accuracy: 0.3125\n",
            "Epoch 80/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6350 - accuracy: 0.6250\n",
            "Epoch 80: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6812 - accuracy: 0.5625 - val_loss: 2.0290 - val_accuracy: 0.3125\n",
            "Epoch 81/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6627 - accuracy: 0.6125\n",
            "Epoch 81: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6790 - accuracy: 0.5781 - val_loss: 1.0413 - val_accuracy: 0.4688\n",
            "Epoch 82/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6777 - accuracy: 0.5795\n",
            "Epoch 82: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6715 - accuracy: 0.5938 - val_loss: 0.9043 - val_accuracy: 0.5625\n",
            "Epoch 83/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6713 - accuracy: 0.6136\n",
            "Epoch 83: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6520 - accuracy: 0.6016 - val_loss: 0.7521 - val_accuracy: 0.5312\n",
            "Epoch 84/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6002 - accuracy: 0.7375\n",
            "Epoch 84: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6098 - accuracy: 0.6953 - val_loss: 1.8845 - val_accuracy: 0.3125\n",
            "Epoch 85/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7002 - accuracy: 0.6125\n",
            "Epoch 85: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6631 - accuracy: 0.6562 - val_loss: 0.7911 - val_accuracy: 0.5625\n",
            "Epoch 86/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6955 - accuracy: 0.5568\n",
            "Epoch 86: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.5625 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
            "Epoch 87/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.5964 - accuracy: 0.6932\n",
            "Epoch 87: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6319 - accuracy: 0.6641 - val_loss: 0.6998 - val_accuracy: 0.5625\n",
            "Epoch 88/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6722 - accuracy: 0.6023\n",
            "Epoch 88: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6579 - accuracy: 0.6016 - val_loss: 0.7343 - val_accuracy: 0.5625\n",
            "Epoch 89/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6485 - accuracy: 0.6591\n",
            "Epoch 89: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6369 - accuracy: 0.6562 - val_loss: 0.7750 - val_accuracy: 0.5625\n",
            "Epoch 90/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7032 - accuracy: 0.5909\n",
            "Epoch 90: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.6016 - val_loss: 0.6444 - val_accuracy: 0.5938\n",
            "Epoch 91/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6684 - accuracy: 0.6375\n",
            "Epoch 91: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6432 - accuracy: 0.6641 - val_loss: 0.7117 - val_accuracy: 0.5625\n",
            "Epoch 92/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7018 - accuracy: 0.5909\n",
            "Epoch 92: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6585 - accuracy: 0.6641 - val_loss: 1.3796 - val_accuracy: 0.3125\n",
            "Epoch 93/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6646 - accuracy: 0.6375\n",
            "Epoch 93: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6541 - accuracy: 0.6172 - val_loss: 0.6658 - val_accuracy: 0.5625\n",
            "Epoch 94/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7074 - accuracy: 0.6023\n",
            "Epoch 94: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6562 - accuracy: 0.6641 - val_loss: 0.6016 - val_accuracy: 0.7500\n",
            "Epoch 95/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6441 - accuracy: 0.5750\n",
            "Epoch 95: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6390 - accuracy: 0.6094 - val_loss: 0.6174 - val_accuracy: 0.6875\n",
            "Epoch 96/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6583 - accuracy: 0.6125\n",
            "Epoch 96: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6483 - accuracy: 0.6328 - val_loss: 0.6248 - val_accuracy: 0.6875\n",
            "Epoch 97/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6395 - accuracy: 0.6364\n",
            "Epoch 97: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6564 - accuracy: 0.6172 - val_loss: 0.6404 - val_accuracy: 0.6875\n",
            "Epoch 98/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6458 - accuracy: 0.6500\n",
            "Epoch 98: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6515 - accuracy: 0.6328 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
            "Epoch 99/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6558 - accuracy: 0.5972\n",
            "Epoch 99: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6268 - accuracy: 0.6719 - val_loss: 0.6108 - val_accuracy: 0.6875\n",
            "Epoch 100/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6356 - accuracy: 0.6375\n",
            "Epoch 100: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6487 - accuracy: 0.6016 - val_loss: 0.7302 - val_accuracy: 0.5625\n",
            "Epoch 101/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6466 - accuracy: 0.5972\n",
            "Epoch 101: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6480 - accuracy: 0.5859 - val_loss: 1.3567 - val_accuracy: 0.3125\n",
            "Epoch 102/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6865 - accuracy: 0.6136\n",
            "Epoch 102: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6890 - accuracy: 0.6172 - val_loss: 0.6007 - val_accuracy: 0.6875\n",
            "Epoch 103/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6604 - accuracy: 0.5972\n",
            "Epoch 103: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6375 - accuracy: 0.6484 - val_loss: 0.5903 - val_accuracy: 0.7500\n",
            "Epoch 104/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6217 - accuracy: 0.7125\n",
            "Epoch 104: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6375 - accuracy: 0.6641 - val_loss: 0.6020 - val_accuracy: 0.6250\n",
            "Epoch 105/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6660 - accuracy: 0.6250\n",
            "Epoch 105: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6502 - accuracy: 0.6562 - val_loss: 0.5864 - val_accuracy: 0.7500\n",
            "Epoch 106/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6323 - accuracy: 0.6500\n",
            "Epoch 106: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6394 - accuracy: 0.6406 - val_loss: 0.5806 - val_accuracy: 0.6875\n",
            "Epoch 107/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6524 - accuracy: 0.5833\n",
            "Epoch 107: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6409 - accuracy: 0.6172 - val_loss: 0.6169 - val_accuracy: 0.6875\n",
            "Epoch 108/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6705 - accuracy: 0.6250\n",
            "Epoch 108: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6679 - accuracy: 0.6250 - val_loss: 0.6092 - val_accuracy: 0.6875\n",
            "Epoch 109/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6298 - accuracy: 0.6250\n",
            "Epoch 109: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6488 - accuracy: 0.6172 - val_loss: 0.6065 - val_accuracy: 0.6875\n",
            "Epoch 110/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6445 - accuracy: 0.6023\n",
            "Epoch 110: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6334 - accuracy: 0.6250 - val_loss: 0.6083 - val_accuracy: 0.6250\n",
            "Epoch 111/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6815 - accuracy: 0.6023\n",
            "Epoch 111: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6565 - accuracy: 0.6250 - val_loss: 0.6647 - val_accuracy: 0.5625\n",
            "Epoch 112/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6693 - accuracy: 0.6477\n",
            "Epoch 112: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6471 - accuracy: 0.6719 - val_loss: 0.5852 - val_accuracy: 0.6875\n",
            "Epoch 113/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6918 - accuracy: 0.6250\n",
            "Epoch 113: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6780 - accuracy: 0.6172 - val_loss: 0.5812 - val_accuracy: 0.7188\n",
            "Epoch 114/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6126 - accuracy: 0.6705\n",
            "Epoch 114: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6376 - accuracy: 0.6250 - val_loss: 0.5841 - val_accuracy: 0.7500\n",
            "Epoch 115/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6407 - accuracy: 0.6375\n",
            "Epoch 115: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6515 - accuracy: 0.6172 - val_loss: 0.5855 - val_accuracy: 0.7500\n",
            "Epoch 116/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6117 - accuracy: 0.6750\n",
            "Epoch 116: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6190 - accuracy: 0.6641 - val_loss: 0.5857 - val_accuracy: 0.7188\n",
            "Epoch 117/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6089 - accuracy: 0.6705\n",
            "Epoch 117: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6150 - accuracy: 0.6641 - val_loss: 0.8784 - val_accuracy: 0.5312\n",
            "Epoch 118/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6209 - accuracy: 0.6477\n",
            "Epoch 118: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6484 - accuracy: 0.6250 - val_loss: 0.6505 - val_accuracy: 0.6875\n",
            "Epoch 119/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6364 - accuracy: 0.6818\n",
            "Epoch 119: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6442 - accuracy: 0.6641 - val_loss: 0.6346 - val_accuracy: 0.6875\n",
            "Epoch 120/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6386 - accuracy: 0.6625\n",
            "Epoch 120: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6370 - accuracy: 0.6562 - val_loss: 0.6167 - val_accuracy: 0.6875\n",
            "Epoch 121/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6419 - accuracy: 0.6500\n",
            "Epoch 121: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6644 - accuracy: 0.5938 - val_loss: 0.6200 - val_accuracy: 0.6875\n",
            "Epoch 122/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6473 - accuracy: 0.5909\n",
            "Epoch 122: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6357 - accuracy: 0.6172 - val_loss: 0.6439 - val_accuracy: 0.6875\n",
            "Epoch 123/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6687 - accuracy: 0.6364\n",
            "Epoch 123: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6594 - accuracy: 0.6250 - val_loss: 0.6404 - val_accuracy: 0.6250\n",
            "Epoch 124/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6124 - accuracy: 0.7045\n",
            "Epoch 124: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6465 - accuracy: 0.6641 - val_loss: 0.6255 - val_accuracy: 0.7188\n",
            "Epoch 125/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6399 - accuracy: 0.6477\n",
            "Epoch 125: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6430 - accuracy: 0.6406 - val_loss: 0.6315 - val_accuracy: 0.6562\n",
            "Epoch 126/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6395 - accuracy: 0.6250\n",
            "Epoch 126: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6125 - accuracy: 0.6484 - val_loss: 0.6396 - val_accuracy: 0.6562\n",
            "Epoch 127/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6314 - accuracy: 0.6000\n",
            "Epoch 127: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6349 - accuracy: 0.5938 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
            "Epoch 128/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6870 - accuracy: 0.5375\n",
            "Epoch 128: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6560 - accuracy: 0.5859 - val_loss: 0.6475 - val_accuracy: 0.6562\n",
            "Epoch 129/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6398 - accuracy: 0.5875\n",
            "Epoch 129: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6583 - accuracy: 0.5859 - val_loss: 0.6228 - val_accuracy: 0.6562\n",
            "Epoch 130/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6650 - accuracy: 0.6172\n",
            "Epoch 130: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6650 - accuracy: 0.6172 - val_loss: 0.6281 - val_accuracy: 0.6875\n",
            "Epoch 131/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6261 - accuracy: 0.6111\n",
            "Epoch 131: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6388 - accuracy: 0.6172 - val_loss: 0.9449 - val_accuracy: 0.3125\n",
            "Epoch 132/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6832 - accuracy: 0.5625\n",
            "Epoch 132: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6346 - accuracy: 0.6406 - val_loss: 1.3357 - val_accuracy: 0.3125\n",
            "Epoch 133/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6352 - accuracy: 0.6750\n",
            "Epoch 133: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6273 - accuracy: 0.6875 - val_loss: 0.7003 - val_accuracy: 0.5625\n",
            "Epoch 134/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6788 - accuracy: 0.5375\n",
            "Epoch 134: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6347 - accuracy: 0.6172 - val_loss: 1.1578 - val_accuracy: 0.3125\n",
            "Epoch 135/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6308 - accuracy: 0.6389\n",
            "Epoch 135: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6062 - accuracy: 0.6562 - val_loss: 1.8928 - val_accuracy: 0.3125\n",
            "Epoch 136/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6359 - accuracy: 0.6136\n",
            "Epoch 136: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6274 - accuracy: 0.6406 - val_loss: 0.6403 - val_accuracy: 0.6562\n",
            "Epoch 137/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5884 - accuracy: 0.6750\n",
            "Epoch 137: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6084 - accuracy: 0.6719 - val_loss: 0.6196 - val_accuracy: 0.6875\n",
            "Epoch 138/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6164 - accuracy: 0.6944\n",
            "Epoch 138: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6473 - accuracy: 0.6641 - val_loss: 0.6169 - val_accuracy: 0.7188\n",
            "Epoch 139/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6570 - accuracy: 0.6000\n",
            "Epoch 139: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6328 - accuracy: 0.6406 - val_loss: 0.6149 - val_accuracy: 0.7188\n",
            "Epoch 140/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6360 - accuracy: 0.6477\n",
            "Epoch 140: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6345 - accuracy: 0.6406 - val_loss: 0.6126 - val_accuracy: 0.7188\n",
            "Epoch 141/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6604 - accuracy: 0.6000\n",
            "Epoch 141: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6295 - accuracy: 0.6562 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
            "Epoch 142/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6225 - accuracy: 0.7083\n",
            "Epoch 142: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6224 - accuracy: 0.6875 - val_loss: 0.7162 - val_accuracy: 0.5625\n",
            "Epoch 143/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6107 - accuracy: 0.7625\n",
            "Epoch 143: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6392 - accuracy: 0.6953 - val_loss: 0.9203 - val_accuracy: 0.4688\n",
            "Epoch 144/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6445 - accuracy: 0.6375\n",
            "Epoch 144: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6459 - accuracy: 0.6328 - val_loss: 0.6681 - val_accuracy: 0.5938\n",
            "Epoch 145/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5959 - accuracy: 0.7000\n",
            "Epoch 145: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6480 - accuracy: 0.6250 - val_loss: 0.6450 - val_accuracy: 0.6250\n",
            "Epoch 146/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6050 - accuracy: 0.6944\n",
            "Epoch 146: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6333 - val_accuracy: 0.6562\n",
            "Epoch 147/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6781 - accuracy: 0.6136\n",
            "Epoch 147: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6636 - accuracy: 0.6172 - val_loss: 0.6837 - val_accuracy: 0.5625\n",
            "Epoch 148/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5685 - accuracy: 0.7750\n",
            "Epoch 148: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6065 - accuracy: 0.7109 - val_loss: 2.3737 - val_accuracy: 0.3125\n",
            "Epoch 149/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6772 - accuracy: 0.5875\n",
            "Epoch 149: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6399 - accuracy: 0.6406 - val_loss: 1.0213 - val_accuracy: 0.3125\n",
            "Epoch 150/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6902 - accuracy: 0.5750\n",
            "Epoch 150: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6643 - accuracy: 0.6094 - val_loss: 1.1837 - val_accuracy: 0.3125\n",
            "Epoch 151/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6857 - accuracy: 0.5750\n",
            "Epoch 151: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6616 - accuracy: 0.5938 - val_loss: 0.6798 - val_accuracy: 0.6562\n",
            "Epoch 152/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6252 - accuracy: 0.6500\n",
            "Epoch 152: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6397 - accuracy: 0.6484 - val_loss: 0.8932 - val_accuracy: 0.4062\n",
            "Epoch 153/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6141 - accuracy: 0.6500\n",
            "Epoch 153: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6230 - accuracy: 0.6406 - val_loss: 0.6261 - val_accuracy: 0.6562\n",
            "Epoch 154/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5648 - accuracy: 0.7125\n",
            "Epoch 154: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6187 - accuracy: 0.6172 - val_loss: 1.1375 - val_accuracy: 0.3125\n",
            "Epoch 155/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6414 - accuracy: 0.6125\n",
            "Epoch 155: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6189 - accuracy: 0.6328 - val_loss: 0.6356 - val_accuracy: 0.6875\n",
            "Epoch 156/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6552 - accuracy: 0.6375\n",
            "Epoch 156: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6582 - accuracy: 0.6406 - val_loss: 0.6462 - val_accuracy: 0.6562\n",
            "Epoch 157/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6092 - accuracy: 0.6944\n",
            "Epoch 157: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5933 - accuracy: 0.7031 - val_loss: 1.3340 - val_accuracy: 0.3125\n",
            "Epoch 158/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6475 - accuracy: 0.6625\n",
            "Epoch 158: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6498 - accuracy: 0.6406 - val_loss: 0.6239 - val_accuracy: 0.6562\n",
            "Epoch 159/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6093 - accuracy: 0.6944\n",
            "Epoch 159: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6238 - accuracy: 0.6562 - val_loss: 1.3103 - val_accuracy: 0.3125\n",
            "Epoch 160/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5864 - accuracy: 0.6250\n",
            "Epoch 160: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6117 - accuracy: 0.5781 - val_loss: 0.6308 - val_accuracy: 0.6875\n",
            "Epoch 161/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6275 - accuracy: 0.6667\n",
            "Epoch 161: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6271 - accuracy: 0.6719 - val_loss: 0.6417 - val_accuracy: 0.6562\n",
            "Epoch 162/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.6273 - accuracy: 0.6875\n",
            "Epoch 162: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6093 - accuracy: 0.6797 - val_loss: 0.7191 - val_accuracy: 0.5625\n",
            "Epoch 163/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6192 - accuracy: 0.6125\n",
            "Epoch 163: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6083 - accuracy: 0.6641 - val_loss: 0.9158 - val_accuracy: 0.5000\n",
            "Epoch 164/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6753 - accuracy: 0.5694\n",
            "Epoch 164: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6603 - accuracy: 0.5469 - val_loss: 0.6362 - val_accuracy: 0.6875\n",
            "Epoch 165/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6004 - accuracy: 0.7000\n",
            "Epoch 165: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6523 - accuracy: 0.6406 - val_loss: 1.1033 - val_accuracy: 0.4062\n",
            "Epoch 166/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5885 - accuracy: 0.6667\n",
            "Epoch 166: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6294 - accuracy: 0.6484 - val_loss: 0.5887 - val_accuracy: 0.7188\n",
            "Epoch 167/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.6523 - accuracy: 0.5781\n",
            "Epoch 167: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6374 - accuracy: 0.6328 - val_loss: 0.5979 - val_accuracy: 0.6875\n",
            "Epoch 168/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6409 - accuracy: 0.6500\n",
            "Epoch 168: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6353 - accuracy: 0.6641 - val_loss: 0.6504 - val_accuracy: 0.6875\n",
            "Epoch 169/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6262 - accuracy: 0.6125\n",
            "Epoch 169: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6591 - accuracy: 0.5859 - val_loss: 0.6457 - val_accuracy: 0.6875\n",
            "Epoch 170/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6519 - accuracy: 0.6125\n",
            "Epoch 170: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6359 - accuracy: 0.6406 - val_loss: 0.6557 - val_accuracy: 0.6875\n",
            "Epoch 171/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6137 - accuracy: 0.6625\n",
            "Epoch 171: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6439 - accuracy: 0.6328 - val_loss: 0.6382 - val_accuracy: 0.6875\n",
            "Epoch 172/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6446 - accuracy: 0.6000\n",
            "Epoch 172: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6252 - accuracy: 0.6250 - val_loss: 0.6158 - val_accuracy: 0.7188\n",
            "Epoch 173/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5917 - accuracy: 0.6625\n",
            "Epoch 173: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6083 - accuracy: 0.6719 - val_loss: 1.9889 - val_accuracy: 0.3125\n",
            "Epoch 174/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6138 - accuracy: 0.6528\n",
            "Epoch 174: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6229 - accuracy: 0.6328 - val_loss: 2.4365 - val_accuracy: 0.3125\n",
            "Epoch 175/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6206 - accuracy: 0.6364\n",
            "Epoch 175: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6302 - accuracy: 0.6484 - val_loss: 0.6115 - val_accuracy: 0.7188\n",
            "Epoch 176/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6701 - accuracy: 0.6000\n",
            "Epoch 176: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6481 - accuracy: 0.6250 - val_loss: 0.6315 - val_accuracy: 0.6875\n",
            "Epoch 177/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6524 - accuracy: 0.6250\n",
            "Epoch 177: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6451 - accuracy: 0.6250 - val_loss: 0.6366 - val_accuracy: 0.6875\n",
            "Epoch 178/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5807 - accuracy: 0.7375\n",
            "Epoch 178: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6006 - accuracy: 0.7031 - val_loss: 0.6282 - val_accuracy: 0.6875\n",
            "Epoch 179/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5494 - accuracy: 0.7361\n",
            "Epoch 179: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5976 - accuracy: 0.6641 - val_loss: 0.7662 - val_accuracy: 0.5312\n",
            "Epoch 180/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5677 - accuracy: 0.6389\n",
            "Epoch 180: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5889 - accuracy: 0.6641 - val_loss: 0.6358 - val_accuracy: 0.6875\n",
            "Epoch 181/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.5946 - accuracy: 0.6932\n",
            "Epoch 181: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6016 - accuracy: 0.6719 - val_loss: 0.6043 - val_accuracy: 0.7188\n",
            "Epoch 182/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6150 - accuracy: 0.6750\n",
            "Epoch 182: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5886 - accuracy: 0.7109 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
            "Epoch 183/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6519 - accuracy: 0.6125\n",
            "Epoch 183: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6579 - accuracy: 0.6250 - val_loss: 0.6189 - val_accuracy: 0.6562\n",
            "Epoch 184/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6156 - accuracy: 0.6875\n",
            "Epoch 184: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5932 - accuracy: 0.7188 - val_loss: 1.8345 - val_accuracy: 0.3125\n",
            "Epoch 185/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6380 - accuracy: 0.6250\n",
            "Epoch 185: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6390 - accuracy: 0.6250 - val_loss: 1.4143 - val_accuracy: 0.3125\n",
            "Epoch 186/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6139 - accuracy: 0.6750\n",
            "Epoch 186: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6275 - accuracy: 0.6797 - val_loss: 1.1326 - val_accuracy: 0.3125\n",
            "Epoch 187/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5223 - accuracy: 0.7917\n",
            "Epoch 187: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5854 - accuracy: 0.7109 - val_loss: 0.6261 - val_accuracy: 0.7188\n",
            "Epoch 188/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.6067 - accuracy: 0.6562\n",
            "Epoch 188: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6110 - accuracy: 0.6641 - val_loss: 0.6252 - val_accuracy: 0.6562\n",
            "Epoch 189/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6186 - accuracy: 0.6667\n",
            "Epoch 189: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5958 - accuracy: 0.6875 - val_loss: 0.9909 - val_accuracy: 0.4062\n",
            "Epoch 190/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6138 - accuracy: 0.6875\n",
            "Epoch 190: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6513 - accuracy: 0.6406 - val_loss: 0.7817 - val_accuracy: 0.5312\n",
            "Epoch 191/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5995 - accuracy: 0.6250\n",
            "Epoch 191: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6550 - accuracy: 0.6172 - val_loss: 0.6203 - val_accuracy: 0.6562\n",
            "Epoch 192/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6212 - accuracy: 0.6875\n",
            "Epoch 192: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6123 - accuracy: 0.6953 - val_loss: 1.3204 - val_accuracy: 0.3125\n",
            "Epoch 193/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5716 - accuracy: 0.6625\n",
            "Epoch 193: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5801 - accuracy: 0.6641 - val_loss: 0.9996 - val_accuracy: 0.3125\n",
            "Epoch 194/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6249 - accuracy: 0.6625\n",
            "Epoch 194: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6092 - accuracy: 0.6875 - val_loss: 0.7009 - val_accuracy: 0.6875\n",
            "Epoch 195/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6085 - accuracy: 0.6250\n",
            "Epoch 195: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6032 - accuracy: 0.6250 - val_loss: 0.7818 - val_accuracy: 0.5312\n",
            "Epoch 196/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6971 - accuracy: 0.5750\n",
            "Epoch 196: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6429 - accuracy: 0.6562 - val_loss: 0.6601 - val_accuracy: 0.6875\n",
            "Epoch 197/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6247 - accuracy: 0.6625\n",
            "Epoch 197: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6030 - accuracy: 0.6797 - val_loss: 0.6053 - val_accuracy: 0.6875\n",
            "Epoch 198/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.5585 - accuracy: 0.7386\n",
            "Epoch 198: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6107 - accuracy: 0.7109 - val_loss: 1.2533 - val_accuracy: 0.3125\n",
            "Epoch 199/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6229 - accuracy: 0.6000\n",
            "Epoch 199: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6221 - accuracy: 0.6094 - val_loss: 0.6256 - val_accuracy: 0.6875\n",
            "Epoch 200/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6400 - accuracy: 0.6375\n",
            "Epoch 200: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6394 - accuracy: 0.6250 - val_loss: 0.6158 - val_accuracy: 0.6875\n",
            "Epoch 201/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6390 - accuracy: 0.6528\n",
            "Epoch 201: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6096 - accuracy: 0.6797 - val_loss: 1.6487 - val_accuracy: 0.3125\n",
            "Epoch 202/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6276 - accuracy: 0.6818\n",
            "Epoch 202: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6200 - accuracy: 0.6641 - val_loss: 1.8342 - val_accuracy: 0.3125\n",
            "Epoch 203/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5923 - accuracy: 0.6875\n",
            "Epoch 203: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5923 - accuracy: 0.6875 - val_loss: 0.6131 - val_accuracy: 0.7188\n",
            "Epoch 204/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6010 - accuracy: 0.7222\n",
            "Epoch 204: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6140 - accuracy: 0.6875 - val_loss: 0.6006 - val_accuracy: 0.6875\n",
            "Epoch 205/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5473 - accuracy: 0.7639\n",
            "Epoch 205: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5932 - accuracy: 0.7266 - val_loss: 1.6667 - val_accuracy: 0.3125\n",
            "Epoch 206/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5678 - accuracy: 0.7125\n",
            "Epoch 206: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5899 - accuracy: 0.6875 - val_loss: 1.7919 - val_accuracy: 0.3125\n",
            "Epoch 207/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5938 - accuracy: 0.6875\n",
            "Epoch 207: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6237 - accuracy: 0.6953 - val_loss: 1.0568 - val_accuracy: 0.3125\n",
            "Epoch 208/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5526 - accuracy: 0.7375\n",
            "Epoch 208: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5566 - accuracy: 0.7266 - val_loss: 0.7497 - val_accuracy: 0.5938\n",
            "Epoch 209/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5477 - accuracy: 0.7750\n",
            "Epoch 209: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5879 - accuracy: 0.7031 - val_loss: 0.7443 - val_accuracy: 0.5938\n",
            "Epoch 210/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5783 - accuracy: 0.7500\n",
            "Epoch 210: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5847 - accuracy: 0.7422 - val_loss: 0.8242 - val_accuracy: 0.5000\n",
            "Epoch 211/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6074 - accuracy: 0.6875\n",
            "Epoch 211: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6112 - accuracy: 0.6875 - val_loss: 0.6572 - val_accuracy: 0.6562\n",
            "Epoch 212/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5942 - accuracy: 0.6625\n",
            "Epoch 212: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5649 - accuracy: 0.7031 - val_loss: 1.6805 - val_accuracy: 0.3125\n",
            "Epoch 213/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6242 - accuracy: 0.6806\n",
            "Epoch 213: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6180 - accuracy: 0.6641 - val_loss: 3.6888 - val_accuracy: 0.3125\n",
            "Epoch 214/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5764 - accuracy: 0.7125\n",
            "Epoch 214: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5794 - accuracy: 0.6953 - val_loss: 2.0202 - val_accuracy: 0.3125\n",
            "Epoch 215/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5684 - accuracy: 0.7375\n",
            "Epoch 215: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5917 - accuracy: 0.7188 - val_loss: 1.8597 - val_accuracy: 0.3125\n",
            "Epoch 216/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6130 - accuracy: 0.6389\n",
            "Epoch 216: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6074 - accuracy: 0.6641 - val_loss: 0.7334 - val_accuracy: 0.6250\n",
            "Epoch 217/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6031 - accuracy: 0.7000\n",
            "Epoch 217: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6050 - accuracy: 0.6797 - val_loss: 0.5981 - val_accuracy: 0.7188\n",
            "Epoch 218/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6299 - accuracy: 0.6625\n",
            "Epoch 218: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6203 - accuracy: 0.6562 - val_loss: 0.6568 - val_accuracy: 0.6875\n",
            "Epoch 219/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6272 - accuracy: 0.6250\n",
            "Epoch 219: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6267 - accuracy: 0.6484 - val_loss: 0.6206 - val_accuracy: 0.7188\n",
            "Epoch 220/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6171 - accuracy: 0.6705\n",
            "Epoch 220: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6074 - accuracy: 0.6875 - val_loss: 0.5978 - val_accuracy: 0.6875\n",
            "Epoch 221/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.5834 - accuracy: 0.6591\n",
            "Epoch 221: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5749 - accuracy: 0.6641 - val_loss: 1.4757 - val_accuracy: 0.3125\n",
            "Epoch 222/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5983 - accuracy: 0.6750\n",
            "Epoch 222: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6194 - accuracy: 0.6484 - val_loss: 0.6010 - val_accuracy: 0.7500\n",
            "Epoch 223/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6069 - accuracy: 0.7250\n",
            "Epoch 223: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6040 - accuracy: 0.7031 - val_loss: 1.4983 - val_accuracy: 0.3125\n",
            "Epoch 224/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5915 - accuracy: 0.6528\n",
            "Epoch 224: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6258 - accuracy: 0.6406 - val_loss: 3.5503 - val_accuracy: 0.3125\n",
            "Epoch 225/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7016 - accuracy: 0.6375\n",
            "Epoch 225: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7124 - accuracy: 0.5703 - val_loss: 3.1386 - val_accuracy: 0.3125\n",
            "Epoch 226/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6917 - accuracy: 0.5500\n",
            "Epoch 226: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6963 - accuracy: 0.5547 - val_loss: 2.4563 - val_accuracy: 0.3125\n",
            "Epoch 227/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7362 - accuracy: 0.5250\n",
            "Epoch 227: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.7011 - accuracy: 0.5625 - val_loss: 2.1692 - val_accuracy: 0.3125\n",
            "Epoch 228/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.7149 - accuracy: 0.6250\n",
            "Epoch 228: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6781 - accuracy: 0.6406 - val_loss: 2.0031 - val_accuracy: 0.3125\n",
            "Epoch 229/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6638 - accuracy: 0.5909\n",
            "Epoch 229: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6499 - accuracy: 0.6172 - val_loss: 1.8097 - val_accuracy: 0.3125\n",
            "Epoch 230/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6605 - accuracy: 0.6500\n",
            "Epoch 230: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6538 - accuracy: 0.6484 - val_loss: 1.6245 - val_accuracy: 0.3125\n",
            "Epoch 231/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6992 - accuracy: 0.5500\n",
            "Epoch 231: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.7056 - accuracy: 0.5469 - val_loss: 1.4599 - val_accuracy: 0.3125\n",
            "Epoch 232/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6833 - accuracy: 0.5500\n",
            "Epoch 232: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6468 - accuracy: 0.6016 - val_loss: 1.3842 - val_accuracy: 0.3125\n",
            "Epoch 233/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6590 - accuracy: 0.6250\n",
            "Epoch 233: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6524 - accuracy: 0.6250 - val_loss: 1.3984 - val_accuracy: 0.3125\n",
            "Epoch 234/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6544 - accuracy: 0.6625\n",
            "Epoch 234: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6581 - accuracy: 0.6406 - val_loss: 1.2885 - val_accuracy: 0.3125\n",
            "Epoch 235/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6910 - accuracy: 0.5375\n",
            "Epoch 235: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6867 - accuracy: 0.5625 - val_loss: 1.1622 - val_accuracy: 0.3125\n",
            "Epoch 236/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6028 - accuracy: 0.6125\n",
            "Epoch 236: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6655 - accuracy: 0.5547 - val_loss: 1.0889 - val_accuracy: 0.3125\n",
            "Epoch 237/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6305 - accuracy: 0.7083\n",
            "Epoch 237: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6545 - accuracy: 0.6406 - val_loss: 1.0547 - val_accuracy: 0.3125\n",
            "Epoch 238/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6646 - accuracy: 0.6125\n",
            "Epoch 238: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6677 - accuracy: 0.5938 - val_loss: 0.9910 - val_accuracy: 0.3125\n",
            "Epoch 239/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.7025 - accuracy: 0.6094\n",
            "Epoch 239: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6791 - accuracy: 0.6016 - val_loss: 0.9082 - val_accuracy: 0.3125\n",
            "Epoch 240/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6411 - accuracy: 0.6389\n",
            "Epoch 240: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6736 - accuracy: 0.5781 - val_loss: 0.8814 - val_accuracy: 0.3125\n",
            "Epoch 241/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6549 - accuracy: 0.5750\n",
            "Epoch 241: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6781 - accuracy: 0.5703 - val_loss: 0.8401 - val_accuracy: 0.3125\n",
            "Epoch 242/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6407 - accuracy: 0.6375\n",
            "Epoch 242: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6652 - accuracy: 0.6094 - val_loss: 0.7846 - val_accuracy: 0.4688\n",
            "Epoch 243/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6667 - accuracy: 0.6375\n",
            "Epoch 243: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6711 - accuracy: 0.6094 - val_loss: 0.7544 - val_accuracy: 0.5312\n",
            "Epoch 244/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6532 - accuracy: 0.6375\n",
            "Epoch 244: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6601 - accuracy: 0.6172 - val_loss: 0.7403 - val_accuracy: 0.5625\n",
            "Epoch 245/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6701 - accuracy: 0.5625\n",
            "Epoch 245: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6550 - accuracy: 0.6172 - val_loss: 0.7326 - val_accuracy: 0.5625\n",
            "Epoch 246/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6796 - accuracy: 0.5833\n",
            "Epoch 246: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6626 - accuracy: 0.6328 - val_loss: 0.7214 - val_accuracy: 0.5625\n",
            "Epoch 247/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.7017 - accuracy: 0.5694\n",
            "Epoch 247: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6913 - accuracy: 0.5859 - val_loss: 0.6903 - val_accuracy: 0.5625\n",
            "Epoch 248/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6618 - accuracy: 0.6528\n",
            "Epoch 248: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6352 - accuracy: 0.6719 - val_loss: 0.6750 - val_accuracy: 0.5938\n",
            "Epoch 249/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6816 - accuracy: 0.5250\n",
            "Epoch 249: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6666 - accuracy: 0.5625 - val_loss: 0.6649 - val_accuracy: 0.6250\n",
            "Epoch 250/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6774 - accuracy: 0.5625\n",
            "Epoch 250: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6590 - accuracy: 0.5703 - val_loss: 0.6526 - val_accuracy: 0.6562\n",
            "Epoch 251/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6923 - accuracy: 0.5909\n",
            "Epoch 251: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6763 - accuracy: 0.6016 - val_loss: 0.6471 - val_accuracy: 0.6562\n",
            "Epoch 252/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6354 - accuracy: 0.6625\n",
            "Epoch 252: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6691 - accuracy: 0.6094 - val_loss: 0.6453 - val_accuracy: 0.6562\n",
            "Epoch 253/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6750 - accuracy: 0.5833\n",
            "Epoch 253: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6858 - accuracy: 0.5547 - val_loss: 0.6433 - val_accuracy: 0.6562\n",
            "Epoch 254/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6989 - accuracy: 0.5750\n",
            "Epoch 254: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6832 - accuracy: 0.5938 - val_loss: 0.6399 - val_accuracy: 0.6562\n",
            "Epoch 255/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6589 - accuracy: 0.5875\n",
            "Epoch 255: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6651 - accuracy: 0.5703 - val_loss: 0.6383 - val_accuracy: 0.6562\n",
            "Epoch 256/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6392 - accuracy: 0.6750\n",
            "Epoch 256: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6705 - accuracy: 0.6172 - val_loss: 0.6375 - val_accuracy: 0.6562\n",
            "Epoch 257/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6591 - accuracy: 0.6000\n",
            "Epoch 257: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6639 - accuracy: 0.6016 - val_loss: 0.6381 - val_accuracy: 0.6250\n",
            "Epoch 258/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6595 - accuracy: 0.6000\n",
            "Epoch 258: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6614 - accuracy: 0.6016 - val_loss: 0.6372 - val_accuracy: 0.6562\n",
            "Epoch 259/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6842 - accuracy: 0.5625\n",
            "Epoch 259: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6679 - accuracy: 0.6094 - val_loss: 0.6382 - val_accuracy: 0.6562\n",
            "Epoch 260/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6769 - accuracy: 0.5694\n",
            "Epoch 260: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6664 - accuracy: 0.5703 - val_loss: 0.6363 - val_accuracy: 0.6250\n",
            "Epoch 261/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7009 - accuracy: 0.5375\n",
            "Epoch 261: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6743 - accuracy: 0.5859 - val_loss: 0.6327 - val_accuracy: 0.6562\n",
            "Epoch 262/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6677 - accuracy: 0.5500\n",
            "Epoch 262: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6693 - accuracy: 0.5547 - val_loss: 0.6340 - val_accuracy: 0.6562\n",
            "Epoch 263/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6682 - accuracy: 0.6250\n",
            "Epoch 263: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6596 - accuracy: 0.6094 - val_loss: 0.6328 - val_accuracy: 0.6562\n",
            "Epoch 264/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6334 - accuracy: 0.6125\n",
            "Epoch 264: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6614 - accuracy: 0.6016 - val_loss: 0.6301 - val_accuracy: 0.6562\n",
            "Epoch 265/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6571 - accuracy: 0.6375\n",
            "Epoch 265: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6694 - accuracy: 0.6172 - val_loss: 0.6267 - val_accuracy: 0.6562\n",
            "Epoch 266/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6513 - accuracy: 0.6375\n",
            "Epoch 266: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6627 - accuracy: 0.6250 - val_loss: 0.6279 - val_accuracy: 0.6562\n",
            "Epoch 267/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6441 - accuracy: 0.6375\n",
            "Epoch 267: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6464 - accuracy: 0.6250 - val_loss: 0.6328 - val_accuracy: 0.6562\n",
            "Epoch 268/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6075 - accuracy: 0.6389\n",
            "Epoch 268: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6605 - accuracy: 0.5703 - val_loss: 0.6307 - val_accuracy: 0.6562\n",
            "Epoch 269/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6840 - accuracy: 0.6111\n",
            "Epoch 269: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6868 - accuracy: 0.6250 - val_loss: 0.6319 - val_accuracy: 0.6562\n",
            "Epoch 270/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6299 - accuracy: 0.6500\n",
            "Epoch 270: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6627 - accuracy: 0.5938 - val_loss: 0.6253 - val_accuracy: 0.6250\n",
            "Epoch 271/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6614 - accuracy: 0.5417\n",
            "Epoch 271: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6635 - accuracy: 0.5781 - val_loss: 0.6259 - val_accuracy: 0.6562\n",
            "Epoch 272/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.7165 - accuracy: 0.5125\n",
            "Epoch 272: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6799 - accuracy: 0.5547 - val_loss: 0.6251 - val_accuracy: 0.6562\n",
            "Epoch 273/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6922 - accuracy: 0.6125\n",
            "Epoch 273: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6607 - accuracy: 0.6250 - val_loss: 0.6225 - val_accuracy: 0.6562\n",
            "Epoch 274/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6873 - accuracy: 0.6125\n",
            "Epoch 274: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6620 - accuracy: 0.6328 - val_loss: 0.6217 - val_accuracy: 0.6562\n",
            "Epoch 275/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6310 - accuracy: 0.6375\n",
            "Epoch 275: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6491 - accuracy: 0.6094 - val_loss: 0.6234 - val_accuracy: 0.6562\n",
            "Epoch 276/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6571 - accuracy: 0.6364\n",
            "Epoch 276: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6612 - accuracy: 0.6250 - val_loss: 0.6259 - val_accuracy: 0.6562\n",
            "Epoch 277/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6234 - accuracy: 0.6625\n",
            "Epoch 277: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6487 - accuracy: 0.6172 - val_loss: 0.6260 - val_accuracy: 0.6562\n",
            "Epoch 278/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6516 - accuracy: 0.5875\n",
            "Epoch 278: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6431 - accuracy: 0.6172 - val_loss: 0.6241 - val_accuracy: 0.6562\n",
            "Epoch 279/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6607 - accuracy: 0.5875\n",
            "Epoch 279: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6584 - accuracy: 0.5859 - val_loss: 0.6215 - val_accuracy: 0.6562\n",
            "Epoch 280/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6701 - accuracy: 0.5341\n",
            "Epoch 280: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6754 - accuracy: 0.5547 - val_loss: 0.6206 - val_accuracy: 0.6250\n",
            "Epoch 281/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6697 - accuracy: 0.6125\n",
            "Epoch 281: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6679 - accuracy: 0.6172 - val_loss: 0.6212 - val_accuracy: 0.6250\n",
            "Epoch 282/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.7084 - accuracy: 0.5833\n",
            "Epoch 282: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6816 - accuracy: 0.5859 - val_loss: 0.6232 - val_accuracy: 0.6562\n",
            "Epoch 283/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6740 - accuracy: 0.5500\n",
            "Epoch 283: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6664 - accuracy: 0.5547 - val_loss: 0.6259 - val_accuracy: 0.6562\n",
            "Epoch 284/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6657 - accuracy: 0.5875\n",
            "Epoch 284: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6575 - accuracy: 0.5781 - val_loss: 0.6240 - val_accuracy: 0.6562\n",
            "Epoch 285/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6453 - accuracy: 0.6528\n",
            "Epoch 285: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6489 - accuracy: 0.6328 - val_loss: 0.6206 - val_accuracy: 0.6562\n",
            "Epoch 286/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6348 - accuracy: 0.6250\n",
            "Epoch 286: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6550 - accuracy: 0.5938 - val_loss: 0.6204 - val_accuracy: 0.6562\n",
            "Epoch 287/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6743 - accuracy: 0.6000\n",
            "Epoch 287: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6670 - accuracy: 0.6172 - val_loss: 0.6217 - val_accuracy: 0.6562\n",
            "Epoch 288/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6429 - accuracy: 0.6125\n",
            "Epoch 288: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6511 - accuracy: 0.6406 - val_loss: 0.6241 - val_accuracy: 0.6562\n",
            "Epoch 289/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.6621 - accuracy: 0.6250\n",
            "Epoch 289: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6605 - accuracy: 0.6328 - val_loss: 0.6247 - val_accuracy: 0.6562\n",
            "Epoch 290/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6481 - accuracy: 0.6750\n",
            "Epoch 290: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6592 - accuracy: 0.6484 - val_loss: 0.6224 - val_accuracy: 0.6562\n",
            "Epoch 291/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6397 - accuracy: 0.6625\n",
            "Epoch 291: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6573 - accuracy: 0.6328 - val_loss: 0.6214 - val_accuracy: 0.6562\n",
            "Epoch 292/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.6548 - accuracy: 0.6562\n",
            "Epoch 292: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6564 - accuracy: 0.5938 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
            "Epoch 293/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6483 - accuracy: 0.6806\n",
            "Epoch 293: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6202 - val_accuracy: 0.6562\n",
            "Epoch 294/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6673 - accuracy: 0.5833\n",
            "Epoch 294: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6670 - accuracy: 0.5938 - val_loss: 0.6221 - val_accuracy: 0.6562\n",
            "Epoch 295/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6923 - accuracy: 0.5125\n",
            "Epoch 295: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6810 - accuracy: 0.5391 - val_loss: 0.6237 - val_accuracy: 0.6875\n",
            "Epoch 296/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6922 - accuracy: 0.5455\n",
            "Epoch 296: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6781 - accuracy: 0.5781 - val_loss: 0.6247 - val_accuracy: 0.6562\n",
            "Epoch 297/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6802 - accuracy: 0.5714\n",
            "Epoch 297: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.6716 - accuracy: 0.6016 - val_loss: 0.6242 - val_accuracy: 0.6562\n",
            "Epoch 298/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6641 - accuracy: 0.5729\n",
            "Epoch 298: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6466 - accuracy: 0.6250 - val_loss: 0.6208 - val_accuracy: 0.6250\n",
            "Epoch 299/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6541 - accuracy: 0.5729\n",
            "Epoch 299: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6582 - accuracy: 0.5469 - val_loss: 0.6205 - val_accuracy: 0.6562\n",
            "Epoch 300/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6696 - accuracy: 0.5833\n",
            "Epoch 300: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6750 - accuracy: 0.5625 - val_loss: 0.6225 - val_accuracy: 0.6562\n",
            "Epoch 301/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6668 - accuracy: 0.5804\n",
            "Epoch 301: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6548 - accuracy: 0.6172 - val_loss: 0.6261 - val_accuracy: 0.6562\n",
            "Epoch 302/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.5391\n",
            "Epoch 302: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6821 - accuracy: 0.5391 - val_loss: 0.6272 - val_accuracy: 0.6562\n",
            "Epoch 303/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6640 - accuracy: 0.5833\n",
            "Epoch 303: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6424 - accuracy: 0.6250 - val_loss: 0.6277 - val_accuracy: 0.6562\n",
            "Epoch 304/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6622 - accuracy: 0.6250\n",
            "Epoch 304: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6622 - accuracy: 0.6250 - val_loss: 0.6263 - val_accuracy: 0.6562\n",
            "Epoch 305/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6525 - accuracy: 0.6250\n",
            "Epoch 305: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6517 - accuracy: 0.6250 - val_loss: 0.6244 - val_accuracy: 0.6562\n",
            "Epoch 306/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6802 - accuracy: 0.5729\n",
            "Epoch 306: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6728 - accuracy: 0.5938 - val_loss: 0.6265 - val_accuracy: 0.6562\n",
            "Epoch 307/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.6465 - accuracy: 0.6058\n",
            "Epoch 307: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6408 - accuracy: 0.6250 - val_loss: 0.6276 - val_accuracy: 0.6562\n",
            "Epoch 308/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6656 - accuracy: 0.6161\n",
            "Epoch 308: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6581 - accuracy: 0.6250 - val_loss: 0.6259 - val_accuracy: 0.6562\n",
            "Epoch 309/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6733 - accuracy: 0.6071\n",
            "Epoch 309: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6685 - accuracy: 0.6172 - val_loss: 0.6262 - val_accuracy: 0.6562\n",
            "Epoch 310/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6494 - accuracy: 0.6083\n",
            "Epoch 310: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6557 - accuracy: 0.6094 - val_loss: 0.6292 - val_accuracy: 0.6562\n",
            "Epoch 311/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6853 - accuracy: 0.5938\n",
            "Epoch 311: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6655 - accuracy: 0.6094 - val_loss: 0.6288 - val_accuracy: 0.6562\n",
            "Epoch 312/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6605 - accuracy: 0.5917\n",
            "Epoch 312: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6651 - accuracy: 0.5781 - val_loss: 0.6276 - val_accuracy: 0.6250\n",
            "Epoch 313/400\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6763 - accuracy: 0.5521\n",
            "Epoch 313: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6548 - accuracy: 0.5938 - val_loss: 0.6255 - val_accuracy: 0.6562\n",
            "Epoch 314/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6648 - accuracy: 0.5859\n",
            "Epoch 314: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6648 - accuracy: 0.5859 - val_loss: 0.6261 - val_accuracy: 0.6250\n",
            "Epoch 315/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6711 - accuracy: 0.5714\n",
            "Epoch 315: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.6681 - accuracy: 0.5781 - val_loss: 0.6272 - val_accuracy: 0.6250\n",
            "Epoch 316/400\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.6291 - accuracy: 0.6538\n",
            "Epoch 316: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6352 - accuracy: 0.6484 - val_loss: 0.6255 - val_accuracy: 0.6250\n",
            "Epoch 317/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6482 - accuracy: 0.6339\n",
            "Epoch 317: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6627 - accuracy: 0.6016 - val_loss: 0.6258 - val_accuracy: 0.6562\n",
            "Epoch 318/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6588 - accuracy: 0.5982\n",
            "Epoch 318: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6465 - accuracy: 0.6250 - val_loss: 0.6271 - val_accuracy: 0.6562\n",
            "Epoch 319/400\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6669 - accuracy: 0.5893\n",
            "Epoch 319: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.6790 - accuracy: 0.5781 - val_loss: 0.6253 - val_accuracy: 0.6562\n",
            "Epoch 320/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6528 - accuracy: 0.6250\n",
            "Epoch 320: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6420 - accuracy: 0.6172 - val_loss: 0.6280 - val_accuracy: 0.6250\n",
            "Epoch 321/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6344 - accuracy: 0.6375\n",
            "Epoch 321: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6435 - accuracy: 0.6172 - val_loss: 0.6324 - val_accuracy: 0.6562\n",
            "Epoch 322/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6696 - accuracy: 0.6250\n",
            "Epoch 322: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6733 - accuracy: 0.6016 - val_loss: 0.6384 - val_accuracy: 0.6250\n",
            "Epoch 323/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6441 - accuracy: 0.5833\n",
            "Epoch 323: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6605 - accuracy: 0.5703 - val_loss: 0.6504 - val_accuracy: 0.5625\n",
            "Epoch 324/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6740 - accuracy: 0.6125\n",
            "Epoch 324: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6469 - accuracy: 0.6484 - val_loss: 0.6689 - val_accuracy: 0.5625\n",
            "Epoch 325/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6518 - accuracy: 0.6375\n",
            "Epoch 325: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6579 - accuracy: 0.6484 - val_loss: 0.7139 - val_accuracy: 0.5625\n",
            "Epoch 326/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6829 - accuracy: 0.5500\n",
            "Epoch 326: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6589 - accuracy: 0.6016 - val_loss: 0.7864 - val_accuracy: 0.4375\n",
            "Epoch 327/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.7265 - accuracy: 0.5000\n",
            "Epoch 327: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6736 - accuracy: 0.5938 - val_loss: 0.7979 - val_accuracy: 0.3750\n",
            "Epoch 328/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6538 - accuracy: 0.6125\n",
            "Epoch 328: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6632 - accuracy: 0.6172 - val_loss: 0.8822 - val_accuracy: 0.3125\n",
            "Epoch 329/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6575 - accuracy: 0.6111\n",
            "Epoch 329: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6586 - accuracy: 0.6172 - val_loss: 1.0724 - val_accuracy: 0.3125\n",
            "Epoch 330/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6707 - accuracy: 0.5556\n",
            "Epoch 330: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6724 - accuracy: 0.5469 - val_loss: 0.9405 - val_accuracy: 0.3125\n",
            "Epoch 331/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6557 - accuracy: 0.6000\n",
            "Epoch 331: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6577 - accuracy: 0.6016 - val_loss: 0.9699 - val_accuracy: 0.3125\n",
            "Epoch 332/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6415 - accuracy: 0.6375\n",
            "Epoch 332: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6433 - accuracy: 0.6172 - val_loss: 0.9066 - val_accuracy: 0.3125\n",
            "Epoch 333/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6150 - accuracy: 0.6750\n",
            "Epoch 333: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6487 - accuracy: 0.6250 - val_loss: 0.9604 - val_accuracy: 0.3125\n",
            "Epoch 334/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6311 - accuracy: 0.7083\n",
            "Epoch 334: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6510 - accuracy: 0.6484 - val_loss: 0.9376 - val_accuracy: 0.3125\n",
            "Epoch 335/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6270 - accuracy: 0.6375\n",
            "Epoch 335: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6528 - accuracy: 0.6094 - val_loss: 0.9418 - val_accuracy: 0.3125\n",
            "Epoch 336/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6321 - accuracy: 0.6667\n",
            "Epoch 336: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6450 - accuracy: 0.6484 - val_loss: 0.7495 - val_accuracy: 0.3750\n",
            "Epoch 337/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6808 - accuracy: 0.5625\n",
            "Epoch 337: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6648 - accuracy: 0.6016 - val_loss: 0.7008 - val_accuracy: 0.5625\n",
            "Epoch 338/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6874 - accuracy: 0.5750\n",
            "Epoch 338: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6755 - accuracy: 0.5703 - val_loss: 0.6708 - val_accuracy: 0.5625\n",
            "Epoch 339/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6112 - accuracy: 0.6750\n",
            "Epoch 339: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6342 - accuracy: 0.6562 - val_loss: 0.6681 - val_accuracy: 0.5625\n",
            "Epoch 340/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6472 - accuracy: 0.5625\n",
            "Epoch 340: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6597 - accuracy: 0.5703 - val_loss: 0.6616 - val_accuracy: 0.5312\n",
            "Epoch 341/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6176 - accuracy: 0.7125\n",
            "Epoch 341: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6485 - accuracy: 0.6641 - val_loss: 0.6614 - val_accuracy: 0.5312\n",
            "Epoch 342/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6483 - accuracy: 0.6500\n",
            "Epoch 342: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6598 - accuracy: 0.6094 - val_loss: 0.6660 - val_accuracy: 0.5625\n",
            "Epoch 343/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6313 - accuracy: 0.6250\n",
            "Epoch 343: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6414 - accuracy: 0.6016 - val_loss: 0.6912 - val_accuracy: 0.5625\n",
            "Epoch 344/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6202 - accuracy: 0.7000\n",
            "Epoch 344: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6463 - accuracy: 0.6484 - val_loss: 0.7617 - val_accuracy: 0.3438\n",
            "Epoch 345/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6517 - accuracy: 0.6000\n",
            "Epoch 345: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6449 - accuracy: 0.6406 - val_loss: 0.7490 - val_accuracy: 0.3438\n",
            "Epoch 346/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6549 - accuracy: 0.6944\n",
            "Epoch 346: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6481 - accuracy: 0.6875 - val_loss: 0.7541 - val_accuracy: 0.3438\n",
            "Epoch 347/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6728 - accuracy: 0.6125\n",
            "Epoch 347: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6491 - accuracy: 0.6484 - val_loss: 0.6946 - val_accuracy: 0.5625\n",
            "Epoch 348/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6292 - accuracy: 0.6389\n",
            "Epoch 348: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6462 - accuracy: 0.6328 - val_loss: 0.5975 - val_accuracy: 0.7188\n",
            "Epoch 349/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6409 - accuracy: 0.6797\n",
            "Epoch 349: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6409 - accuracy: 0.6797 - val_loss: 0.6102 - val_accuracy: 0.6562\n",
            "Epoch 350/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6428 - accuracy: 0.6094\n",
            "Epoch 350: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6428 - accuracy: 0.6094 - val_loss: 0.6666 - val_accuracy: 0.5312\n",
            "Epoch 351/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6282 - accuracy: 0.6528\n",
            "Epoch 351: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6171 - accuracy: 0.6797 - val_loss: 0.6397 - val_accuracy: 0.5625\n",
            "Epoch 352/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6417 - accuracy: 0.6000\n",
            "Epoch 352: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6491 - accuracy: 0.6016 - val_loss: 0.5904 - val_accuracy: 0.7188\n",
            "Epoch 353/400\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.6630 - accuracy: 0.6023\n",
            "Epoch 353: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6562 - accuracy: 0.6094 - val_loss: 0.6571 - val_accuracy: 0.5312\n",
            "Epoch 354/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.6484\n",
            "Epoch 354: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6511 - accuracy: 0.6484 - val_loss: 0.6773 - val_accuracy: 0.5625\n",
            "Epoch 355/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6663 - accuracy: 0.5417\n",
            "Epoch 355: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6675 - accuracy: 0.5781 - val_loss: 0.6212 - val_accuracy: 0.6250\n",
            "Epoch 356/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6524 - accuracy: 0.6875\n",
            "Epoch 356: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6579 - accuracy: 0.6172 - val_loss: 0.6296 - val_accuracy: 0.5938\n",
            "Epoch 357/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6260 - accuracy: 0.6389\n",
            "Epoch 357: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6364 - accuracy: 0.6250 - val_loss: 0.6032 - val_accuracy: 0.6875\n",
            "Epoch 358/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6644 - accuracy: 0.6016\n",
            "Epoch 358: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6644 - accuracy: 0.6016 - val_loss: 0.5936 - val_accuracy: 0.7188\n",
            "Epoch 359/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6480 - accuracy: 0.6667\n",
            "Epoch 359: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6383 - accuracy: 0.6562 - val_loss: 0.6476 - val_accuracy: 0.5625\n",
            "Epoch 360/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6412 - accuracy: 0.6250\n",
            "Epoch 360: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6412 - accuracy: 0.6250 - val_loss: 0.6529 - val_accuracy: 0.5312\n",
            "Epoch 361/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.7036 - accuracy: 0.5278\n",
            "Epoch 361: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6717 - accuracy: 0.5781 - val_loss: 0.6252 - val_accuracy: 0.6875\n",
            "Epoch 362/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6320 - accuracy: 0.6500\n",
            "Epoch 362: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6495 - accuracy: 0.6328 - val_loss: 0.6638 - val_accuracy: 0.6562\n",
            "Epoch 363/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6391 - accuracy: 0.6484\n",
            "Epoch 363: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6391 - accuracy: 0.6484 - val_loss: 0.6325 - val_accuracy: 0.6875\n",
            "Epoch 364/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6546 - accuracy: 0.6125\n",
            "Epoch 364: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6634 - accuracy: 0.5859 - val_loss: 0.6010 - val_accuracy: 0.7188\n",
            "Epoch 365/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.6797\n",
            "Epoch 365: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6380 - accuracy: 0.6797 - val_loss: 0.5995 - val_accuracy: 0.7188\n",
            "Epoch 366/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6840 - accuracy: 0.6250\n",
            "Epoch 366: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6573 - accuracy: 0.6328 - val_loss: 0.7210 - val_accuracy: 0.5312\n",
            "Epoch 367/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6445 - accuracy: 0.6000\n",
            "Epoch 367: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6546 - accuracy: 0.5938 - val_loss: 0.7505 - val_accuracy: 0.4375\n",
            "Epoch 368/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6776 - accuracy: 0.6250\n",
            "Epoch 368: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6564 - accuracy: 0.6562 - val_loss: 0.7260 - val_accuracy: 0.4688\n",
            "Epoch 369/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6641 - accuracy: 0.5938\n",
            "Epoch 369: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6641 - accuracy: 0.5938 - val_loss: 0.6855 - val_accuracy: 0.6250\n",
            "Epoch 370/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6275 - accuracy: 0.6625\n",
            "Epoch 370: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6546 - accuracy: 0.6016 - val_loss: 0.6517 - val_accuracy: 0.6562\n",
            "Epoch 371/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6840 - accuracy: 0.6111\n",
            "Epoch 371: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6501 - accuracy: 0.6484 - val_loss: 0.6151 - val_accuracy: 0.7188\n",
            "Epoch 372/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6655 - accuracy: 0.5750\n",
            "Epoch 372: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6561 - accuracy: 0.5938 - val_loss: 0.6139 - val_accuracy: 0.6562\n",
            "Epoch 373/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6975 - accuracy: 0.5000\n",
            "Epoch 373: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6517 - accuracy: 0.6016 - val_loss: 0.6289 - val_accuracy: 0.5938\n",
            "Epoch 374/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6051 - accuracy: 0.6944\n",
            "Epoch 374: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6209 - accuracy: 0.6562 - val_loss: 0.6364 - val_accuracy: 0.5625\n",
            "Epoch 375/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6233 - accuracy: 0.6641\n",
            "Epoch 375: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6233 - accuracy: 0.6641 - val_loss: 0.7875 - val_accuracy: 0.5625\n",
            "Epoch 376/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.7055 - accuracy: 0.5694\n",
            "Epoch 376: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6628 - accuracy: 0.6172 - val_loss: 0.7795 - val_accuracy: 0.5625\n",
            "Epoch 377/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6672 - accuracy: 0.6528\n",
            "Epoch 377: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6500 - accuracy: 0.6406 - val_loss: 0.6772 - val_accuracy: 0.5625\n",
            "Epoch 378/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6856 - accuracy: 0.6000\n",
            "Epoch 378: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6456 - accuracy: 0.6406 - val_loss: 0.6307 - val_accuracy: 0.6250\n",
            "Epoch 379/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6590 - accuracy: 0.6528\n",
            "Epoch 379: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6348 - accuracy: 0.6875 - val_loss: 0.6418 - val_accuracy: 0.5938\n",
            "Epoch 380/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6470 - accuracy: 0.5833\n",
            "Epoch 380: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6522 - accuracy: 0.6094 - val_loss: 0.8205 - val_accuracy: 0.5625\n",
            "Epoch 381/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6040 - accuracy: 0.7000\n",
            "Epoch 381: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6415 - accuracy: 0.6484 - val_loss: 0.6103 - val_accuracy: 0.6562\n",
            "Epoch 382/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6444 - accuracy: 0.6375\n",
            "Epoch 382: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6349 - accuracy: 0.6484 - val_loss: 0.6066 - val_accuracy: 0.7500\n",
            "Epoch 383/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6157 - accuracy: 0.6667\n",
            "Epoch 383: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6315 - accuracy: 0.6719 - val_loss: 0.6343 - val_accuracy: 0.6562\n",
            "Epoch 384/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6426 - accuracy: 0.6667\n",
            "Epoch 384: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6351 - accuracy: 0.6641 - val_loss: 0.6187 - val_accuracy: 0.7500\n",
            "Epoch 385/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6681 - accuracy: 0.5972\n",
            "Epoch 385: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6580 - accuracy: 0.6016 - val_loss: 0.6283 - val_accuracy: 0.6250\n",
            "Epoch 386/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6150 - accuracy: 0.7000\n",
            "Epoch 386: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6352 - accuracy: 0.6562 - val_loss: 0.9663 - val_accuracy: 0.4062\n",
            "Epoch 387/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6465 - accuracy: 0.6375\n",
            "Epoch 387: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6607 - accuracy: 0.6172 - val_loss: 0.6259 - val_accuracy: 0.6875\n",
            "Epoch 388/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6456 - accuracy: 0.6250\n",
            "Epoch 388: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6334 - accuracy: 0.6328 - val_loss: 0.6497 - val_accuracy: 0.6562\n",
            "Epoch 389/400\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.6250\n",
            "Epoch 389: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6461 - accuracy: 0.6250 - val_loss: 0.6303 - val_accuracy: 0.6250\n",
            "Epoch 390/400\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.6573 - accuracy: 0.6406\n",
            "Epoch 390: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6465 - accuracy: 0.6250 - val_loss: 0.6927 - val_accuracy: 0.5625\n",
            "Epoch 391/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6270 - accuracy: 0.6875\n",
            "Epoch 391: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6428 - accuracy: 0.6641 - val_loss: 0.6376 - val_accuracy: 0.5938\n",
            "Epoch 392/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5773 - accuracy: 0.7361\n",
            "Epoch 392: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6292 - accuracy: 0.6875 - val_loss: 0.6191 - val_accuracy: 0.6875\n",
            "Epoch 393/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6254 - accuracy: 0.5875\n",
            "Epoch 393: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6297 - accuracy: 0.6250 - val_loss: 0.6093 - val_accuracy: 0.7500\n",
            "Epoch 394/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6161 - accuracy: 0.6389\n",
            "Epoch 394: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6236 - accuracy: 0.6328 - val_loss: 0.7413 - val_accuracy: 0.4375\n",
            "Epoch 395/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6365 - accuracy: 0.7125\n",
            "Epoch 395: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6570 - accuracy: 0.6797 - val_loss: 0.8084 - val_accuracy: 0.4688\n",
            "Epoch 396/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6628 - accuracy: 0.6375\n",
            "Epoch 396: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6674 - accuracy: 0.6328 - val_loss: 0.7497 - val_accuracy: 0.4375\n",
            "Epoch 397/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6404 - accuracy: 0.6667\n",
            "Epoch 397: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6360 - accuracy: 0.6641 - val_loss: 0.6619 - val_accuracy: 0.6562\n",
            "Epoch 398/400\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.6628 - accuracy: 0.6000\n",
            "Epoch 398: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6531 - accuracy: 0.6406 - val_loss: 0.6209 - val_accuracy: 0.7500\n",
            "Epoch 399/400\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.6674 - accuracy: 0.6528\n",
            "Epoch 399: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6651 - accuracy: 0.6484 - val_loss: 0.6109 - val_accuracy: 0.7188\n",
            "Epoch 400/400\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6320 - accuracy: 0.6750\n",
            "Epoch 400: val_accuracy did not improve from 0.78125\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6328 - accuracy: 0.6797 - val_loss: 0.6223 - val_accuracy: 0.6562\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7dc6110550>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, Y_test, batch_size = 8)[1])) "
      ],
      "metadata": {
        "id": "atv0zeHCy0WF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0eb4bbc-f977-4a8b-b910-eea3eebd384c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7805\n",
            "\n",
            " 테스트 정확도: 0.7805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KkCaiIDpZLO2"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}